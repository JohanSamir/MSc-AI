# -*- coding: utf-8 -*-
"""DQN-Classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EciLN-uKG5gcC7yb8cFQWEP-n2KjO-V0
"""

from collections import deque, defaultdict, namedtuple
import numpy as np
import pandas as pd
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym
import random
import os


from google.colab import drive 
drive.mount('/content/drive')
path = '/content/drive/My Drive/SaveFiles/Data/'

import sys
sys.path.append(path)

class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, seed):
        """
        Build a fully connected neural network
        
        Parameters
        ----------
        state_size (int): State dimension
        action_size (int): Action dimension
        seed (int): random seed
        """
        super(QNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        self.fc1 = nn.Linear(state_size, 128) #Applies a linear transformation to the incoming data
        self.fc2 = nn.Linear(128, 128)

        self.fc3 = nn.Linear(128, action_size)
        
    def forward(self, x):
        """Forward pass"""
        x = F.relu(self.fc1(x)) #Applies the rectified linear unit function element-wise
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

class ReplayBuffer:
    def __init__(self, hyper_pa, seed):
        """
        Replay memory allow agent to record experiences and learn from them
        
        Parametes
        ---------
        buffer_size (int): maximum size of internal memory
        batch_size (int): sample size from experience
        seed (int): random seed
        """

        self.hyper_pa = hyper_pa
        self.batch_size = self.hyper_pa['BATCH_SIZE']
        self.seed = random.seed(seed)
        self.memory = deque(maxlen=self.hyper_pa['BUFFER_SIZE'])# internal memory (deque)
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])
    
    def add(self, state, action, reward, next_state, done):
        """Add experience"""
        experience = self.experience(state, action, reward, next_state, done)
        self.memory.append(experience)
                
    def sample(self):
        """ 
        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors 
        """
        experiences = random.sample(self.memory, k=self.batch_size)

        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(self.hyper_pa['DEVICE'])
        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(self.hyper_pa['DEVICE'])        
        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(self.hyper_pa['DEVICE'])        
        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(self.hyper_pa['DEVICE']) 
        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(self.hyper_pa['DEVICE'])  
        
        return (states, actions, rewards, next_states, dones)
        
    def __len__(self):
        return len(self.memory)

class DQNAgent:
    def __init__(self, state_size, action_size, hyper_pa, seed):
        """
        DQN Agent interacts with the environment, 
        stores the experience and learns from it
        
        Parameters
        ----------
        state_size (int): Dimension of state
        action_size (int): Dimension of action
        seed (int): random seed
        """
        self.hyper_pa = hyper_pa
        self.state_size = state_size
        self.action_size = action_size
        self.seed = random.seed(seed)
        # Initialize Q and Fixed Q networks
        self.q_network = QNetwork(state_size, action_size, seed).to(self.hyper_pa['DEVICE'])
        self.fixed_network = QNetwork(state_size, action_size, seed).to(self.hyper_pa['DEVICE'])
        #optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

        self.optimizer = optim.Adam(self.q_network.parameters())
        # Initiliase memory 
        self.memory = ReplayBuffer(hyper_pa, seed)
        self.timestep = 0
                
    def reestar(self):
      for name, module in self.q_network.named_children():
          print('resetting ', name)
          module.reset_parameters()

    def load_model(self):
        path = os.path.join('/content/drive/My Drive/SaveFiles/Data/net.pth')
        state_dict = torch.load(path)
        self.q_network.load_state_dict(state_dict)
        print('loading')
          

    def step(self, state, action, reward, next_state, done):
        """
        Update Agent's knowledge
        
        Parameters
        ----------
        state (array_like): Current state of environment
        action (int): Action taken in current state
        reward (float): Reward received after taking action 
        next_state (array_like): Next state returned by the environment after taking action
        done (bool): whether the episode ended after taking action
        """
        self.memory.add(state, action, reward, next_state, done)
        self.timestep += 1
        if self.timestep % self.hyper_pa['UPDATE_EVERY'] == 0:
            if len(self.memory) > self.hyper_pa['BATCH_SIZE']:
                sampled_experiences = self.memory.sample()
                self.learn(sampled_experiences)
        
    def learn(self, experiences):
        """
        Learn from experience by training the q_network 
        
        Parameters
        ----------
        experiences (array_like): List of experiences sampled from agent's memory
        """
        states, actions, rewards, next_states, dones = experiences
        
        if self.hyper_pa['DQN'] == True:
          action_values = self.fixed_network(next_states).detach()
          max_action_values = action_values.max(1)[0].unsqueeze(1)
          #print('max_action_values:',max_action_values)
          Q_target = rewards + (self.hyper_pa['GAMMA'] * max_action_values * (1 - dones))
          Q_expected = self.q_network(states).gather(1, actions)
          # Calculate loss
          #loss = F.mse_loss(Q_expected, Q_target)  # I do not undertand?
          loss = F.mse_loss(Q_target,Q_expected) #Change [Joh+]
          self.optimizer.zero_grad()
          # backward pass
          loss.backward()
          # update weights
          self.optimizer.step()
          # Update fixed weights (ecery cte times update the fixed weights of the target network)
          self.update_fixed_network(self.q_network, self.fixed_network)
          
        elif self.hyper_pa['DoubleDQN'] == True:
          q_values  = self.q_network(states)
          next_q_values = self.q_network(next_states)
          next_q_state_values = self.fixed_network(next_states) 
          q_value  = q_values.gather(1, actions).squeeze(1) 
          next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)
          reward = rewards.squeeze(1)
          done = dones.squeeze(1)
          expected_q_value = reward + self.hyper_pa['GAMMA'] * next_q_value * (1 - done)
          loss = (q_value - (expected_q_value.data)).pow(2).mean()
          self.optimizer.zero_grad()
          # backward pass
          loss.backward()
          # update weights
          self.optimizer.step()
          # Update fixed weights (ecery cte times update the fixed weights of the target network)
          self.update_fixed_network(self.q_network, self.fixed_network)

        else:
          print('Seleccione DQN or Double DQN')
    
    def update_fixed_network(self, q_network, fixed_network):
        """
        Update fixed network by copying weights from Q network using TAU param
        
        Parameters
        ----------
        q_network (PyTorch model): Q network
        fixed_network (PyTorch model): Fixed target network
        """
        self.fixed_network.load_state_dict(self.q_network.state_dict())
        
        
    def act(self, state, eps=0.0):
        """
        Choose the action
        Parameters
        ----------
        state (array_like): current state of environment
        eps (float): epsilon for epsilon-greedy action selection
        """
        rnd = random.random()
        if rnd < eps:
            return np.random.randint(self.action_size)
        else:
            state = torch.from_numpy(state).float().unsqueeze(0).to(self.hyper_pa['DEVICE'])
            # set the network into evaluation mode 
            self.q_network.eval()
            with torch.no_grad():
                action_values = self.q_network(state)
            # Back to training mode
            self.q_network.train()
            action = np.argmax(action_values.cpu().data.numpy())
            return action    
        
    def checkpoint(self, filename):
        torch.save(self.q_network.state_dict(), filename)