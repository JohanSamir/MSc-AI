# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1At2OktJQOmE8WLL5_tfG0qVqXlGUdnUQ
"""

#!apt install swig
#!pip install box2d box2d-kengz

# Commented out IPython magic to ensure Python compatibility.
import random
import sys
from time import time
from collections import deque, defaultdict, namedtuple
import numpy as np
import pandas as pd
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym
import matplotlib.pyplot as plt
import os
import seaborn as sns

# %matplotlib inline

#plt.style.use('seaborn')
plt.style.use('fivethirtyeight')

from google.colab import drive 
drive.mount('/content/drive')
path = '/content/drive/My Drive/SaveFiles/Data/'

import sys
sys.path.append(path)

#import wrappers
from dqn_classes import *

hyper_pa = {'ENV':"CartPole-v0",
            'BUFFER_SIZE':int(1e5),
            'BATCH_SIZE': int(32),
            'GAMMA': 0.99 ,
            'UPDATE_EVERY':4,
            'MAX_EPISODES':700, 
            'MAX_STEPS': 200,
            'ENV_SOLVED': 200,
            'PRINT_EVERY': 100,
            'EPS_START': 1.0,
            'EPS_DECAY': 0.999,
            'EPS_MIN': 0.01,
            'DEVICE': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),
            'TRIALS':3,
            'DQN':True,
            'DoubleDQN':False
            }

env = gym.make(hyper_pa['ENV'])
obs = env.reset()

state_size = env.observation_space.shape[0]
action_size = env.action_space.n
dqn_agent = DQNAgent(state_size, action_size, hyper_pa, seed=0)

# Train 
scoresT = []
running_rewardsT = []

for i in range (0,hyper_pa['TRIALS']):
  start = time()
  
  #dqn_agent.reestar()
  dqn_agent.load_model()
  # Maintain a list of last 100 scores
  scores_window = deque(maxlen=100)
  eps = hyper_pa['EPS_START']

  running_reward = None
  running_rewards = []
  scores = []
  running_rewards = []

  print('Trial No:', i)
  for episode in range(1, hyper_pa['MAX_EPISODES'] + 1): # from 1 to  MAX_EPISODES + 1, 0 simulation was executed above.
      state = env.reset()
      score = 0
      for t in range(hyper_pa['MAX_STEPS']):
          action = dqn_agent.act(state, eps) 
          next_state, reward, done, info = env.step(action)   
          dqn_agent.step(state, action, reward, next_state, done)
          state = next_state        
          score += reward
          
          if done: # Break for if the environment is the task is resolved
              break
              
          eps = max(eps * hyper_pa['EPS_DECAY'], hyper_pa['EPS_MIN']) #set the exploration/ explotation value for taking an action
          if episode % hyper_pa['PRINT_EVERY'] == 0:
              mean_score = np.mean(scores_window)
              print('\r Progress {}/{}, average score:{:.2f}'.format(episode, hyper_pa['MAX_EPISODES'], mean_score), end="")
          if score >= hyper_pa['ENV_SOLVED']:
              mean_score = np.mean(scores_window)
              print('\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end="")
              #sys.stdout.flush()
              #dqn_agent.checkpoint(path)
              break
              
      scores_window.append(score) # I am not sure if it has been used.
      scores.append(score)
      scor = np.array(scores).reshape(len(scores),1)

      running_reward = np.mean(scores[-100:])
      running_rewards.append(running_reward)
      runi = np.array(running_rewards).reshape(len(running_rewards),1)

  if i ==0:
      scoresT = scor
      running_rewardsT = runi
  else:
    scoresT = np.hstack((scoresT,scor))
    running_rewardsT = np.hstack((running_rewardsT,runi))


  end = time()    
  print('Took {} seconds'.format(end - start))

print('Finished!')

scoresTT = np.transpose(scoresT)
running_rewardsTT = np.transpose(running_rewardsT)
print(scoresTT.shape, running_rewardsTT.shape)

df = pd.DataFrame(running_rewardsTT).melt()
print(df.head())
sns.lineplot(x="variable", y="value", data=df)

plt.figure(figsize=(10,6))
plt.plot(scoresTT[0,:])
plt.plot(running_rewardsTT[0,:])
plt.plot(scoresTT[1,:],'c')
plt.plot(running_rewardsTT[1,:])
plt.plot(scoresTT[2,:],'y')
plt.plot(running_rewardsTT[2,:])

plt.title('DQN Training')
plt.xlabel('# of episodes')
plt.ylabel('score')
plt.show()