{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It’s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn’t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  6\n",
      "State size  500\n",
      "qtable: (500, 6)\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size \", state_size)\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print('qtable:', qtable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 50000        # Total episodes\n",
    "total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99    #99            # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "#for episode in range(2):\n",
    "\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    #print('episode:', episode)\n",
    "    #print('state:', state)\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "\n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *\n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        #print('\\nqtable:',qtable)\n",
    "\n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n",
    "        # If done : finish episode\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 8.45\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    #print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        # env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        total_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2st Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    ## Initialise starting data\n",
    "    def __init__(self):\n",
    "        # Set information about the gridworld\n",
    "        self.height = 5\n",
    "        self.width = 5\n",
    "        self.grid = np.zeros(( self.height, self.width)) - 1\n",
    "        \n",
    "        # Set random start location for the agent\n",
    "        self.current_location = ( 4, np.random.randint(0,5))\n",
    "        \n",
    "        # Set locations for the bomb and the gold\n",
    "        self.bomb_location = (1,3)\n",
    "        self.gold_location = (0,3)\n",
    "        self.terminal_states = [ self.bomb_location, self.gold_location]\n",
    "        \n",
    "        # Set grid rewards for special cells\n",
    "        self.grid[ self.bomb_location[0], self.bomb_location[1]] = -10\n",
    "        self.grid[ self.gold_location[0], self.gold_location[1]] = 10\n",
    "        \n",
    "        # Set available actions\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "        \n",
    "    ## Put methods here:\n",
    "    def get_available_actions(self):\n",
    "        \"\"\"Returns possible actions\"\"\"\n",
    "        return self.actions\n",
    "    \n",
    "    def agent_on_map(self):\n",
    "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
    "        grid = np.zeros(( self.height, self.width))\n",
    "        grid[ self.current_location[0], self.current_location[1]] = 1\n",
    "        return grid\n",
    "    \n",
    "    def get_reward(self, new_location):\n",
    "        \"\"\"Returns the reward for an input position\"\"\"\n",
    "        return self.grid[ new_location[0], new_location[1]]\n",
    "        \n",
    "    \n",
    "    def make_step(self, action):\n",
    "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
    "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
    "        # Store previous location\n",
    "        last_location = self.current_location\n",
    "        \n",
    "        # UP\n",
    "        if action == 'UP':\n",
    "            # If agent is at the top, stay still, collect reward\n",
    "            if last_location[0] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] - 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "        \n",
    "        # DOWN\n",
    "        elif action == 'DOWN':\n",
    "            # If agent is at bottom, stay still, collect reward\n",
    "            if last_location[0] == self.height - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] + 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "            \n",
    "        # LEFT\n",
    "        elif action == 'LEFT':\n",
    "            # If agent is at the left, stay still, collect reward\n",
    "            if last_location[1] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] - 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "\n",
    "        # RIGHT\n",
    "        elif action == 'RIGHT':\n",
    "            # If agent is at the right, stay still, collect reward\n",
    "            if last_location[1] == self.width - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] + 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                \n",
    "        return reward\n",
    "    \n",
    "    def check_state(self):\n",
    "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
    "        if self.current_location in self.terminal_states:\n",
    "            return 'TERMINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    # Intialise\n",
    "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "        self.q_table = dict() # Store all Q-values in dictionary of dictionaries \n",
    "        for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
    "            for y in range(environment.width):\n",
    "                self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def choose_action(self, available_actions):\n",
    "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
    "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
    "        else:\n",
    "            q_values_of_state = self.q_table[self.environment.current_location]\n",
    "            maxValue = max(q_values_of_state.values())\n",
    "            action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
    "        q_values_of_state = self.q_table[new_state]\n",
    "        max_q_value_in_new_state = max(q_values_of_state.values())\n",
    "        current_q_value = self.q_table[old_state][action]\n",
    "        \n",
    "        self.q_table[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n",
    "        #return self.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
    "#def play(environment, agent, trials=2, max_steps_per_episode=3, learn=False):\n",
    "    print(learn)\n",
    "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
    "    reward_per_episode = [] # Initialise performance log\n",
    "    \n",
    "    for trial in range(trials): # Run trials\n",
    "        cumulative_reward = 0 # Initialise values of each game\n",
    "        step = 0\n",
    "        game_over = False\n",
    "        while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
    "            old_state = environment.current_location\n",
    "            action = agent.choose_action(environment.actions) \n",
    "            reward = environment.make_step(action)\n",
    "            new_state = environment.current_location\n",
    "            \n",
    "            if learn == True: # Update Q-values if learning is specified\n",
    "                agent.learn(old_state, reward, new_state, action)\n",
    "                #print('r:',r)\n",
    "                \n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            \n",
    "            if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
    "                environment.__init__()\n",
    "                game_over = True     \n",
    "                \n",
    "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
    "        \n",
    "    return reward_per_episode # Return performance log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the agent = (4, 1)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "Available_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
      "Randomly chosen action = RIGHT\n",
      "Reward obtained = -1.0\n",
      "Current position of the agent = (4, 2)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = Q_Agent(env)\n",
    "\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)\n",
    "reward = env.make_step(chosen_action)\n",
    "print(\"Reward obtained =\", reward)\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f75c10075f8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note the learn=True argument!\n",
    "reward_per_episode = play(env, agent, trials=500, learn=True)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "\tUP\n",
      "\t\t8.999999999999982\n",
      "\tRIGHT\n",
      "\t\t-4.0951\n",
      "\tLEFT\n",
      "\t\t-0.3727996313046343\n",
      "\tDOWN\n",
      "\t\t2.731673486406832\n",
      "(3, 2)\n",
      "\tUP\n",
      "\t\t6.9999999999999725\n",
      "\tRIGHT\n",
      "\t\t-0.7668211438659766\n",
      "\tLEFT\n",
      "\t\t0.36207953247396013\n",
      "\tDOWN\n",
      "\t\t2.2884586987655346\n",
      "(0, 0)\n",
      "\tUP\n",
      "\t\t-0.30000000000000004\n",
      "\tRIGHT\n",
      "\t\t-0.08235005464377529\n",
      "\tLEFT\n",
      "\t\t-0.30000000000000004\n",
      "\tDOWN\n",
      "\t\t-0.327\n",
      "(3, 0)\n",
      "\tUP\n",
      "\t\t-1.1704720316019046\n",
      "\tRIGHT\n",
      "\t\t-1.1539132418479565\n",
      "\tLEFT\n",
      "\t\t-1.2000000000000006\n",
      "\tDOWN\n",
      "\t\t-1.1202096379344613\n",
      "(0, 4)\n",
      "\tUP\n",
      "\t\t0.081\n",
      "\tRIGHT\n",
      "\t\t0.4225795110000001\n",
      "\tLEFT\n",
      "\t\t7.458134171671\n",
      "\tDOWN\n",
      "\t\t-0.11000000000000001\n",
      "(1, 4)\n",
      "\tUP\n",
      "\t\t3.015318121967611\n",
      "\tRIGHT\n",
      "\t\t-0.18725887900000002\n",
      "\tLEFT\n",
      "\t\t-1.0\n",
      "\tDOWN\n",
      "\t\t-0.22900000000000004\n",
      "(1, 3)\n",
      "\tUP\n",
      "\t\t0\n",
      "\tRIGHT\n",
      "\t\t0\n",
      "\tLEFT\n",
      "\t\t0\n",
      "\tDOWN\n",
      "\t\t0\n",
      "(2, 3)\n",
      "\tUP\n",
      "\t\t-1.9\n",
      "\tRIGHT\n",
      "\t\t-0.6678937100000001\n",
      "\tLEFT\n",
      "\t\t4.531116732085034\n",
      "\tDOWN\n",
      "\t\t-0.78264611378\n",
      "(2, 1)\n",
      "\tUP\n",
      "\t\t-0.5967730752909154\n",
      "\tRIGHT\n",
      "\t\t2.5804741593780327\n",
      "\tLEFT\n",
      "\t\t-0.7216045900000001\n",
      "\tDOWN\n",
      "\t\t-0.6723635687000001\n",
      "(2, 4)\n",
      "\tUP\n",
      "\t\t0.10875870731685436\n",
      "\tRIGHT\n",
      "\t\t-0.7000000000000002\n",
      "\tLEFT\n",
      "\t\t-0.6077461900000001\n",
      "\tDOWN\n",
      "\t\t-0.6888274920000002\n",
      "(4, 2)\n",
      "\tUP\n",
      "\t\t5.999999999999967\n",
      "\tRIGHT\n",
      "\t\t0.9643280217103304\n",
      "\tLEFT\n",
      "\t\t1.8866569388935641\n",
      "\tDOWN\n",
      "\t\t0.8009599966780343\n",
      "(1, 0)\n",
      "\tUP\n",
      "\t\t-0.5276100000000001\n",
      "\tRIGHT\n",
      "\t\t-0.4733899532030001\n",
      "\tLEFT\n",
      "\t\t-0.5000000000000001\n",
      "\tDOWN\n",
      "\t\t-0.5634018690000001\n",
      "(0, 3)\n",
      "\tUP\n",
      "\t\t0\n",
      "\tRIGHT\n",
      "\t\t0\n",
      "\tLEFT\n",
      "\t\t0\n",
      "\tDOWN\n",
      "\t\t0\n",
      "(4, 0)\n",
      "\tUP\n",
      "\t\t-1.561054429294006\n",
      "\tRIGHT\n",
      "\t\t3.998897554332522\n",
      "\tLEFT\n",
      "\t\t-1.5000000000000009\n",
      "\tDOWN\n",
      "\t\t-1.2322007890375268\n",
      "(0, 1)\n",
      "\tUP\n",
      "\t\t-0.1\n",
      "\tRIGHT\n",
      "\t\t6.6753728987947705\n",
      "\tLEFT\n",
      "\t\t-0.1\n",
      "\tDOWN\n",
      "\t\t-0.11000000000000001\n",
      "(3, 3)\n",
      "\tUP\n",
      "\t\t-1.1378080205264454\n",
      "\tRIGHT\n",
      "\t\t-1.1996659152378573\n",
      "\tLEFT\n",
      "\t\t3.1214409515329953\n",
      "\tDOWN\n",
      "\t\t-1.1750585179923032\n",
      "(4, 1)\n",
      "\tUP\n",
      "\t\t-0.523204475994123\n",
      "\tRIGHT\n",
      "\t\t4.999999923020435\n",
      "\tLEFT\n",
      "\t\t0.6106985421038389\n",
      "\tDOWN\n",
      "\t\t0.5768977852101567\n",
      "(3, 1)\n",
      "\tUP\n",
      "\t\t-1.0177683284610002\n",
      "\tRIGHT\n",
      "\t\t4.094479465679634\n",
      "\tLEFT\n",
      "\t\t-0.9993729013000001\n",
      "\tDOWN\n",
      "\t\t-1.1038800994820224\n",
      "(4, 4)\n",
      "\tUP\n",
      "\t\t-1.8488910244582175\n",
      "\tRIGHT\n",
      "\t\t-1.523206258706328\n",
      "\tLEFT\n",
      "\t\t3.9992502772396863\n",
      "\tDOWN\n",
      "\t\t-1.9000000000000012\n",
      "(0, 2)\n",
      "\tUP\n",
      "\t\t4.159712342075113\n",
      "\tRIGHT\n",
      "\t\t9.999999999999995\n",
      "\tLEFT\n",
      "\t\t1.4686796980760692\n",
      "\tDOWN\n",
      "\t\t4.068707192620259\n",
      "(2, 0)\n",
      "\tUP\n",
      "\t\t-0.8425458210000002\n",
      "\tRIGHT\n",
      "\t\t-0.5559661410274356\n",
      "\tLEFT\n",
      "\t\t-0.7981\n",
      "\tDOWN\n",
      "\t\t-0.7936545186920002\n",
      "(4, 3)\n",
      "\tUP\n",
      "\t\t-0.755950133241621\n",
      "\tRIGHT\n",
      "\t\t0.03513580943964567\n",
      "\tLEFT\n",
      "\t\t4.99999984686915\n",
      "\tDOWN\n",
      "\t\t-1.1220837632220972\n",
      "(2, 2)\n",
      "\tUP\n",
      "\t\t7.999999999999978\n",
      "\tRIGHT\n",
      "\t\t0.9729831754204882\n",
      "\tLEFT\n",
      "\t\t-0.28756499342087255\n",
      "\tDOWN\n",
      "\t\t2.1908650141120023\n",
      "(3, 4)\n",
      "\tUP\n",
      "\t\t-1.1911958026686453\n",
      "\tRIGHT\n",
      "\t\t-1.3000000000000007\n",
      "\tLEFT\n",
      "\t\t-1.2597497725544673\n",
      "\tDOWN\n",
      "\t\t-1.3213019530559293\n",
      "(1, 1)\n",
      "\tUP\n",
      "\t\t1.525871960856848\n",
      "\tRIGHT\n",
      "\t\t-0.29000000000000004\n",
      "\tLEFT\n",
      "\t\t-0.30000000000000004\n",
      "\tDOWN\n",
      "\t\t-0.45268620000000015\n"
     ]
    }
   ],
   "source": [
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "\n",
    "pretty(agent.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
